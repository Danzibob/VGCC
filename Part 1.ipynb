{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ac57592-5d7f-4671-a2f8-81cc931a644f",
   "metadata": {},
   "source": [
    "# Part 1 - Introduction to images, meshes and perspective projection\n",
    "\n",
    "We're going to make use of quite a few different libraries in our lil crash course. We'll also need to grab some extra assets such as images and 3D models.\n",
    "\n",
    "Today we'll be using numpy, matplotlib, OpenCV and a library for reading 3D meshes in PLY format.\n",
    "\n",
    "If you haven't already, install the required modules using the requirements.txt file, and ensure this notebook is using the right virtual environment\n",
    "\n",
    "The following code block downloads an image and mesh for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef4901c",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Download an image\n",
    "!wget https://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/York_Minster_from_M%26S.JPG/440px-York_Minster_from_M%26S.JPG -P ./assets/\n",
    "\n",
    "# Download a mesh in ply format\n",
    "!wget https://people.sc.fsu.edu/~jburkardt/data/ply/shark.ply -P ./assets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d54ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from plyfile import PlyData, PlyElement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d996ed46",
   "metadata": {},
   "source": [
    "Now let's load an image using OpenCV and see what it looks like using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a7a6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image using OpenCV\n",
    "img = cv2.imread('assets/440px-York_Minster_from_M&S.JPG') \n",
    "print('Image dimensions:',img.shape)\n",
    "print('Image datatype:',img.dtype)\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c889e",
   "metadata": {},
   "source": [
    "So far, so easy. We can see that cv2.imread has returned a numpy array of size 330 pixels high, 440 pixels wide and with three colour channels. \n",
    "We can also see that the values in the image are 8 bit unsigned integers. \n",
    "This is the most common way to store an image and means that for each pixel, you will have three numbers in the range 0..255 representing the intensity for each colour channel.\n",
    "\n",
    "However, the astute among you may have noticed something off about the colour of the image we've displayed...\n",
    "OpenCV decided to use the convention that colour channels are ordered: blue, green, red (BGR) rather than the (now) much more common convention: red, green, blue (RGB). \n",
    "If you're using any other library to process colour images, you probably need to convert to RGB. \n",
    "Let's do that and then display the image again using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2155b35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert BGR to RGB\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "# Display image\n",
    "plt.imshow(img_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfc2195",
   "metadata": {},
   "source": [
    "For clarity, we can now index values in the image as `img_rgb[row,column,colour_channel]` so that, for example, the red, green and blue values at the top left pixel are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe8e2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the individual pixel values from the 3D array\n",
    "print('Red value at pixel (0,0):',img_rgb[0,0,0])\n",
    "print('Green value at pixel (0,0):',img_rgb[0,0,1])\n",
    "print('Blue value at pixel (0,0):',img_rgb[0,0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8381d7cc",
   "metadata": {},
   "source": [
    "Representing image data as integers in the range 0..255 is less convenient when we want to start to manipulate the values in some way. \n",
    "The use of integers will introduce rounding errors that will accumulate over multiple operations. \n",
    "Therefore, we often want to convert image data to floating point values in the range 0..1 with 0 representing no light and 1 the maximum that can be recorded/displayed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd78973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_float = img_rgb.astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d76338",
   "metadata": {},
   "source": [
    "Now, we can apply arbitrary maths to the pixel values in our image. For example, we can multiply by an arbitrary factor to change the brightness of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1add59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_float * 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615394ab",
   "metadata": {},
   "source": [
    "The warning message is interesting here. \n",
    "Because we multiplied the image by 1.5, we now have values that are >1. \n",
    "`imshow` \"clips\" these values to 1, i.e. it uses `min(img_rgb,1.0)`. \n",
    "This simulates \"saturation\" in a camera. \n",
    "It can't record anymore light than 1. \n",
    "As each colour channel reaches 1, you end up with `(1.0,1.0,1.0)` for the three colour channels which means white. \n",
    "Hence, the blue sky has turned white."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741387fd",
   "metadata": {},
   "source": [
    "## Task 1: Manipulating Images [6]\n",
    "\n",
    "Let's do some more experimentation! Try scaling by different values and observe what happens; how about if you scale each colour channel by a different value? What does the image look like if, at each pixel, the value is the same for the three colour channels? (e.g. you might copy the red channel into green and blue, or take the average over the three channels). Can you think why it looks like this? What does it look like if you set two of the colour channels to zero?\n",
    "\n",
    "Take a look through this [OpenCV tutorial](https://docs.opencv.org/4.x/d3/df2/tutorial_py_basic_ops.html) which goes over some of the above plus a bit more.\n",
    "\n",
    "Can you *threshold* the image? This means turning an image into a binary image. For example, you might want to threshold the red channel on 0.5. So values $<0.5$ get set to 0 and $\\geq 0.5$ get set to 1. To display such an image with imshow, you might want to pass `cmap='gray'` to get the expected appearance.\n",
    "\n",
    "Finally, try cropping the image by selecting specific ranges of pixels.\n",
    "\n",
    "If you have other ideas, try them out in the final box!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab986007",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3)\n",
    "fig.set_size_inches(12,6)\n",
    "\n",
    "# Try scaling each channel differently to change the warmth of the image\n",
    "img_warm = ...\n",
    "img_cold = ...\n",
    "\n",
    "# Make a greyscale image by copying or combining the channels\n",
    "img_gray = ...\n",
    "\n",
    "# Threshold the image so we get a black & white mask. e.g. try to mask out the sky\n",
    "img_mask = ...\n",
    "\n",
    "# Crop the image around the minster\n",
    "img_crop = ...\n",
    "\n",
    "# Free space! Be creative :)\n",
    "img_free = ...\n",
    "\n",
    "axs[0,0].imshow(img_warm)\n",
    "axs[0,1].imshow(img_cold)\n",
    "axs[0,2].imshow(img_gray)\n",
    "axs[1,0].imshow(img_mask, cmap='gray')\n",
    "axs[1,1].imshow(img_crop)\n",
    "axs[1,2].imshow(img_free)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2ee0d2",
   "metadata": {},
   "source": [
    "## Projecting and drawing meshes\n",
    "\n",
    "Perspective projection is a transformation we use to translate 3D real-world coordinates into 2D pixel coordinates on our camera sensor.\n",
    "\n",
    "Here we model a simple pinhole camera so we can see the transformations we need to describe:\n",
    "\n",
    "\n",
    "![Perspective projection diagram](images/perspective_projection.png \"Perspective projection\")\n",
    "\n",
    "_N.B: It is normal to model the \"image plane\" as being in front of the camera (i.e. between the camera and the object) rather than behind the focal point of the lens, as this makes some of the maths and reasoning easier, and means we don't have to invert the image etc._\n",
    "\n",
    "In this image we have two coordinate systems we want to convert between - the real world coordinates `(u,v,w)` and the image or pixel coordinates `(x,y)`\n",
    "\n",
    "By taking our \"optical center\" as the center of our world coordinate system, it makes it easy to reason about these transformations.\n",
    "\n",
    "We can show that the matrix for this transformation is as follows, where $f$ is the combined scale factor based on the focal distance and the size of the sensor\n",
    "\n",
    "$$\n",
    "\\mathbf{K} = \\begin{bmatrix} f & 0 & c_x \\\\ 0 & f & c_y \\\\ 0 & 0 & 1\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb3a755",
   "metadata": {},
   "source": [
    "In order to have more than one perspective on an object, we may want to move the camera around the world - or rather, the world around the camera...\n",
    "If we want to change our view, we actually apply transformations first to the points we're looking at, _then_ apply the intrinsic camera properties matrix $\\mathbf{K}$.\n",
    "\n",
    "\n",
    "To do this we use a rotation matrix $\\mathbf{R}$ and a translation vector $\\mathbf{t}$ which we can compose together in a homogeneous transformation like so:\n",
    "\n",
    "$$\n",
    "\\lambda  \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} = \n",
    "\\begin{bmatrix} \\mathbf{K} & \\mathbf{0} \\end{bmatrix}\n",
    "\\begin{bmatrix} \\mathbf{R} & \\mathbf{t} \\\\ \\mathbf{0}^T & 1\\end{bmatrix}\n",
    "\\begin{bmatrix} u \\\\ v \\\\ w \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here, $\\lambda$ represents the scalar required to convert back after the homogeneous transformation\n",
    "\n",
    "We can actually simplify this further, and let's use $\\mathbf{\\tilde{x}}$ to represent pixel coordinates and $\\mathbf{\\tilde{w}}$ for world coordinates:\n",
    "\n",
    "$$\n",
    "\\lambda \\mathbf{\\tilde{x}} = \n",
    "\\mathbf{K}\n",
    "\\begin{bmatrix} \\mathbf{R} & \\mathbf{t}\\end{bmatrix}\n",
    "\\mathbf{\\tilde{w}}\n",
    "$$\n",
    "\n",
    "Because we can compose these matricies beforehand we operate on our world points, this means we can end up with a single linear transformation we can apply to a set of points that represent both the camera properties, and its relative position and orientation in the world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc50aa9e",
   "metadata": {},
   "source": [
    "## Task 2: Perspective Projection [5]\n",
    "\n",
    "Now I would like you to compute the perspective projection of the vertices of the mesh into 2D (you can ignore the faces for now - we're just treating it like a point cloud).\n",
    "I've given you the intrinsic and extrinsic camera parameters to perform a reasonable projection into an image that is 500 pixels wide and 326 pixels tall.\n",
    "\n",
    "You need to write some code to apply the perspective projection transformation to our list of points `V` using the `K`, `R` and `t` matrices I've given.\n",
    "\n",
    "_Hint: remember that you need to use homogeneous coordinates for the 3D vertices, do the matrix multiplications, then homogenise._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca49e125",
   "metadata": {},
   "source": [
    "The following code block loads a 3D triangle mesh containing `nvertices` vertices and `nfaces` triangle faces. Vertices are stored in the 3 $\\times$ `nvertices` numpy array `V`. So `V[:,0]` contains the 3D position of the first vertex. Faces are stored in the `nfaces` $\\times$ 3 numpy array `F`. The three vertex indices for the first triangle are stored in `F[0,:]`. Putting those together, the 3D coordinates of the first vertex in the first triangle would be given by `V[:,F[0,:]]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c83e851",
   "metadata": {},
   "outputs": [],
   "source": [
    "plydata = PlyData.read('assets/shark.ply')\n",
    "\n",
    "nvertices = len(plydata['vertex'])\n",
    "nfaces = len(plydata['face'])\n",
    "\n",
    "# Copy vertex positions into numpy array\n",
    "V = np.empty((3,nvertices))\n",
    "V[0,:] = plydata['vertex']['x']\n",
    "V[1,:] = plydata['vertex']['y']\n",
    "V[2,:] = plydata['vertex']['z']\n",
    "\n",
    "# Copy face data into numpy array\n",
    "F = np.vstack(plydata['face'].data['vertex_indices'])\n",
    "\n",
    "# Preview data\n",
    "print('Vertex 0:',V[:,0])\n",
    "print('Vertex 1:',V[:,1])\n",
    "print('Vertex 2:',V[:,2])\n",
    "print('Face 0:',F[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a424d3",
   "metadata": {},
   "source": [
    "Face 0 here is the tuple `(0,1,2)`, which tells us that there is a face made by joining the vertexes 0, 1 and 2.\n",
    "This also tells us the _orientation_ of the face - i.e. which side of the face is outwards.\n",
    "We can use this information in graphics applications to avoid having to render the back of faces and various other optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f185da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are some camera paramters I have provided so we can get a nice looking output for the file we're using\n",
    "# We will fiddle with them later, but leave them as they are until you get the point cloud rendering working!\n",
    "\n",
    "# K is the intrinsic parameters of the camera\n",
    "# it is made up of the focal length/scale factor (2290)\n",
    "# and the center of projection x,y (250, 163)\n",
    "K = np.array([[2290.0,    0.0, 250.0],\n",
    "              [   0.0, 2290.0, 163.0],\n",
    "              [   0.0,    0.0,   1.0]])\n",
    "\n",
    "# R and T are the extrinsic parameters of the camera\n",
    "# together they represent the _pose_ of the camera\n",
    "# R represents a rotation\n",
    "R = np.array([[0.9063, 0.0000, 0.4226],\n",
    "              [0.0000,-1.0000, 0.0000],\n",
    "              [0.4226, 0.0000,-0.9063]])\n",
    "\n",
    "# T represents a translation in world coordinates\n",
    "t = np.array([[1.0350], [0.3861], [15.0140]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d170486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform perspective projection\n",
    "# your output u should be list of 2d points, resulting from transforming the verticies V\n",
    "\n",
    "# Your code goes here:\n",
    "# u = ...\n",
    "\n",
    "# hint: @ operator is matrix multiplication, not *\n",
    "# using * will perform element-wise multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0202307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot projected 2D point cloud\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(6,4)\n",
    "ax.scatter(u[0,:], u[1,:], s=1)\n",
    "ax.axis('equal')\n",
    "ax.set(xlim=(0, 550), ylim=(0, 326))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "print(\"Friend!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c612d711",
   "metadata": {},
   "source": [
    "## Task 3: Working with Triangles [5]\n",
    "\n",
    "Next, I'd like you to draw our friendo here as a wireframe. In other words, you should draw a line in 2D for every triangle edge. To draw a line from `(x1,y1)` to `(x2,y2)` just use `ax.plot([x1,x2],[y1,y2])`. You might want to play with some parameters of `plot` to make the wireframe look nice. I've started you off with a loop over triangles. For each triangle you're going to draw three lines in 2D using the projected vertex positions `u[0,:]` and `u[1,:]` that you computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4f6b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw wireframe\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(nfaces):\n",
    "  # Draw the three triangle edges\n",
    "  # Your code goes here:\n",
    "  # ...\n",
    "  \n",
    "ax.axis('equal')\n",
    "ax.set(xlim=(0, 550), ylim=(0, 326))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e5e398",
   "metadata": {},
   "source": [
    "The observant amongst you may have noticed that the above code is wasteful. Edges are shared between triangles so every edge will have been draw twice. There are algorithms for extracting a list of unique edges from a triangle list. If you are inspired, go and find one of those algorithms and implement it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a7f7b5",
   "metadata": {},
   "source": [
    "## Task 4: projection parameters [4]\n",
    "\n",
    "Now that you have code for projecting a mesh to 2D and displaying it in 2D, try playing with some of the parameters. I carefully chose $\\mathbf{K}$, $\\mathbf{R}$ and $\\mathbf{t}$ so that the model is in a nice pose and fits exactly in the image. \n",
    "\n",
    "First try playing with the focal length, i.e. elements $(1,1)$ and $(2,2)$ in $\\mathbf{K}$ (remember that when we talk about element positions in matrices we give them as (row,column) indices and start from 1 even though our python code will use zero indexing). Try making it bigger and smaller. What happens? How about if the two focal length values are not equal? Next, what happens if you change $c_x$ and $c_y$? (elements $(1,3)$ and $(2,3)$ in $\\mathbf{K}$).\n",
    "\n",
    "Second try playing with the extrinsic parameters. Can you rotate the object around the vertical axis? (You'll need to apply a y axis rotation to your vertices). For extra challenge, can you find out how to do an animation in matplotlib and get the mesh to continuously rotate?\n",
    "\n",
    "Finally, you might like to experiment with some other meshes. [This page](https://people.sc.fsu.edu/~jburkardt/data/ply/ply.html) has loads in PLY format.\n",
    "\n",
    "Copy the code from above and leave your results below, commenting what you changed and why"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6772e2fb",
   "metadata": {},
   "source": [
    "## Total Mark [20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
